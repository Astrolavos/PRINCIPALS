{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2de7dab-86a6-4eb4-ba01-bf24bfc15eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AutoEncoder\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70834236-c1c6-4569-8697-c36b7e66e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627256ee-8c71-4536-be05-2a618fcf4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extract_features.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358cef3-c3d6-424b-aba1-0f73609b47ef",
   "metadata": {},
   "source": [
    "## Input Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb37f66-bedb-4a29-b95e-90760edca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_astrolavos_data(directory, min_samples=1000):\n",
    "    files = glob.glob(f\"{directory}/*\")\n",
    "    res = {}\n",
    "    for f in files:\n",
    "        id = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "        with open(f, 'rb') as f:\n",
    "            loaded_data = np.array(pickle.load(f))\n",
    "        if loaded_data.shape[0] < min_samples:\n",
    "            print(f\"{f} not enough data\")\n",
    "        else:\n",
    "            res[id] = loaded_data\n",
    "            print(id, loaded_data.shape[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ccd99b-2ac4-48cd-a83d-afbbfaa49ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = get_astrolavos_data(\"/data/thomas/Principals/FL/deployment/iotlab_data_filtered\")\n",
    "raw_data = get_astrolavos_data2(\"/data/thomas/Principals/FL/deployment/iotlab_data_bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3257fa-441d-4537-a0d0-d956ec4c8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data, pca_enabled=True, max_iter=sys.maxsize):\n",
    "    res = {}\n",
    "    num_samples = int(min([data[id].shape[0] for id in data.keys()]) / 2)\n",
    "    print(f\"using {num_samples} points for each device\") \n",
    "    test_data = {}\n",
    "    train_data = {}\n",
    "    labels = {}\n",
    "    for id in data.keys():\n",
    "        print(f\"Getting train/test data for {id}\")\n",
    "        train_data[id], test_data[id], _, labels[id] = train_test_split(data[id], np.full(len(data[id]), id), test_size=num_samples, train_size=num_samples, random_state=42)\n",
    "        print(f\"test data for {id} {train_data[id].shape}\")\n",
    "        print(f\"train data for {id} {test_data[id].shape}\")\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "    res = {}\n",
    "    print(\"Generating train/test split\")\n",
    "    for id in tqdm(data.keys()):\n",
    "        res[id] = {}\n",
    "        res[id][\"train_data\"] = train_data[id]\n",
    "        res[id][\"test_data\"] = test_data[id]\n",
    "        res[id][\"test_labels\"] = labels[id]\n",
    "        for id2 in data.keys():\n",
    "            if id2 != id:\n",
    "                res[id][\"test_data\"] = np.concatenate((res[id][\"test_data\"], test_data[id2]), axis=0)\n",
    "                res[id][\"test_labels\"] = np.concatenate((res[id][\"test_labels\"], labels[id2]), axis=0)\n",
    "        scaler = StandardScaler(copy=False)\n",
    "        print(f\"Standardizing data for {id} ..\")\n",
    "        res[id][\"train_data\"] = scaler.fit_transform(res[id][\"train_data\"])\n",
    "        res[id][\"test_data\"] = scaler.transform(res[id][\"test_data\"])\n",
    "        pca_component_num = 100\n",
    "        if pca_enabled==True:\n",
    "            print(f\"using PCA with {pca_component_num} components\")\n",
    "            pca = PCA(copy=True, iterated_power='auto', n_components=pca_component_num, random_state=None, whiten=False, svd_solver='auto', tol=0.0)\n",
    "            #pca.fit(res[id][\"train_data\"])\n",
    "            res[id][\"train_data\"] = pca.fit_transform(res[id][\"train_data\"])\n",
    "            res[id][\"test_data\"] = pca.transform(res[id][\"test_data\"])\n",
    "    print(\"Done\")\n",
    "\n",
    "    return res\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b1ca04-4916-436e-a0a1-af0f0f0c1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_new(data, pca_enabled=True, max_iter=sys.maxsize):\n",
    "    res = {}\n",
    "    num_samples = int(min([data[id].shape[0] for id in data.keys()]) / 2)\n",
    "    print(f\"using {num_samples} points for each device\") \n",
    "    test_data = {}\n",
    "    train_data = {}\n",
    "    labels = {}\n",
    "    for id in data.keys():\n",
    "        print(f\"Getting train/test data for {id}\")\n",
    "        train_data[id], test_data[id], _, labels[id] = train_test_split(data[id], np.full(len(data[id]), id), test_size=num_samples, train_size=num_samples, random_state=42)\n",
    "        print(f\"test data for {id} {train_data[id].shape}, train data for {id} {test_data[id].shape}\")\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "    res = {}\n",
    "    print(\"Generating train/test split\")\n",
    "    for id in data.keys():\n",
    "        res[id] = {}\n",
    "        res[id][\"train_data\"] = train_data[id]\n",
    "        res[id][\"test_data\"] = test_data[id]\n",
    "        res[id][\"test_labels\"] = labels[id]\n",
    "        \n",
    "    #fit on standard scaler and PCA on full training data\n",
    "    \n",
    "    full_training_data = np.empty((0, data[id].shape[1]))\n",
    "    for id in data.keys():\n",
    "        full_training_data = np.concatenate((full_training_data, res[id][\"train_data\"]), axis=0)\n",
    "    print(f\"Concatenated training data has shape {full_training_data.shape}\")\n",
    "    \n",
    "    full_test_data = np.empty((0, data[id].shape[1]))\n",
    "    full_test_labels = np.array([])\n",
    "    for id in data.keys():\n",
    "        full_test_data = np.concatenate((full_test_data, res[id][\"test_data\"]), axis=0)\n",
    "        full_test_labels =  np.concatenate((full_test_labels, res[id][\"test_labels\"]), axis=0)\n",
    "    print(f\"Concatenated test data has shape {full_test_data.shape}\")\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit(full_training_data)\n",
    "    \n",
    "    output = {}\n",
    "    print(\"Scaling data...\")\n",
    "    for id in data.keys():\n",
    "        output[id] = {}\n",
    "        output[id][\"train_data\"] = scaler.transform(res[id][\"train_data\"])\n",
    "        output[id][\"test_data\"] = scaler.transform(full_test_data)\n",
    "        output[id][\"test_labels\"] = full_test_labels\n",
    "    \n",
    "    print(\"Scaling done\")\n",
    "    if pca_enabled == True:\n",
    "        print(\"PCA enabled\")\n",
    "        pca_component_num = 100\n",
    "        pca = PCA(copy=True, iterated_power='auto', n_components=pca_component_num, random_state=None, whiten=False, svd_solver='auto', tol=0.0)\n",
    "        pca.fit(full_training_data)\n",
    "        for id in data.keys():\n",
    "            output[id][\"train_data\"] = pca.transform(output[id][\"train_data\"])\n",
    "            output[id][\"test_data\"] = pca.transform(output[id][\"test_data\"])\n",
    "        \n",
    "    return output, full_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf5e44-addf-4e50-8370-343ad6508abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_d = data_preprocess(raw_data, pca_enabled=False)\n",
    "#input_d, full_training_data = data_preprocess_new(raw_data, pca_enabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c19ab5-effe-4609-a501-8dc566cc46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOSS_FUNC = nn.KLDivLoss\n",
    "# LOSS_FUNC = nn.BCELoss()\n",
    "\n",
    "def get_local_threshold(model, train_data):\n",
    "    device='cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse = list()\n",
    "    threshold_func = LOSS_FUNC()\n",
    "    #threshold_func = nn.MSELoss(reduction=\"none\")\n",
    "    for batch_idx, x in enumerate(train_data):\n",
    "        x = x.to(device).float()\n",
    "        diff = threshold_func(model(x), x)\n",
    "        mse.append(diff)\n",
    "    mse_global = torch.cat(mse).mean(dim=1)\n",
    "    threshold_global = torch.mean(mse_global) + 3* torch.std(mse_global)\n",
    "    return threshold_global\n",
    "\n",
    "\n",
    "res = {}\n",
    "for id in input_d.keys():\n",
    "    res[id] = {}\n",
    "    #input_size = input_d[id]['train_data'].shape[1]\n",
    "    #input_size=input_d[id]['train_data'].shape[1]\n",
    "    input_size=100\n",
    "    sample_num = input_d[id]['train_data'].shape[0]\n",
    "    print(f\"input size {input_size}, num samples {sample_num}\")\n",
    "    model = AutoEncoder(input_size)\n",
    "    learning_rate =  0.03#0.03\n",
    "    device='cpu'\n",
    "    epochs = 3\n",
    "    batch_size = 64\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion = LOSS_FUNC().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    epoch_loss = []\n",
    "    train_data = torch.utils.data.DataLoader(input_d[id]['train_data'][:,:input_size], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "        for batch_idx, x in tqdm(enumerate(train_data), total=5000//batch_size):\n",
    "            x = x.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            decode = model(x)\n",
    "            loss = criterion(decode, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "        print(\"Client \\tEpoch: {}\\tLoss: {:.6f}\".format(epoch, sum(epoch_loss) / len(epoch_loss))) \n",
    "    threshold = get_local_threshold(model, train_data)\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    threshold_func = LOSS_FUNC()\n",
    "\n",
    "    for i in set(input_d[id]['test_labels']):\n",
    "        \n",
    "        idx = np.where(input_d[id]['test_labels'] == i)\n",
    "        test_data = input_d[id]['test_data'][idx]\n",
    "        test_data = torch.utils.data.DataLoader(test_data[:,:input_size], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        same = 0\n",
    "        different = 0\n",
    "        for batch_idx, x in enumerate(test_data):\n",
    "            x = x.to(device).float()\n",
    "            diff = threshold_func(model(x), x)\n",
    "            mse = diff.mean(dim=1)\n",
    "            different += sum(mse > threshold)\n",
    "            same += sum(mse <= threshold)\n",
    "        print(f\"train({id}) vs test({i}): same class={same.numpy()}, different class={different.numpy()}\")\n",
    "        print(\"--------------------------\")\n",
    "        res[id][i] = same.numpy()/ sample_num\n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff568c0-3e2c-4455-bcfd-7e1ea1adc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'NestCamera'\n",
    "res[id] = {}\n",
    "input_size = input_d[id]['train_data'].shape[1]\n",
    "input_size=100\n",
    "sample_num = input_d[id]['train_data'].shape[0]\n",
    "print(f\"input size {input_size}, num samples {sample_num}\")\n",
    "model = AutoEncoder(input_size)\n",
    "learning_rate =  0.03#0.03\n",
    "device='cpu'\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epoch_loss = []\n",
    "train_data = torch.utils.data.DataLoader(input_d[id]['train_data'][:,:input_size], batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e083e6-678b-465c-80c9-5e7774d9ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    batch_loss = []\n",
    "    for batch_idx, x in tqdm(enumerate(train_data)):\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        decode = model(x)\n",
    "        loss = criterion(decode, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "    epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "    print(\"Client \\tEpoch: {}\\tLoss: {:.6f}\".format(epoch, sum(epoch_loss) / len(epoch_loss))) \n",
    "threshold = get_local_threshold(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111df72-d591-4ad2-861d-f2a2108c82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(res.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8825a53-f708-4c05-9c71-597a6ae4664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted_dict:\n",
    "    sorted_dict[i] = dict(sorted(sorted_dict[i].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff75fe6-86fa-42fc-9754-066434881f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b5ea5-6ad9-44b8-a0fb-2ce706c7df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.heatmap(df,annot=True)\n",
    "myplot.set_xticklabels(myplot.get_xticklabels(), rotation=90)\n",
    "plt.xlabel(\"Train Device\")\n",
    "plt.ylabel(\"Test Device\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/data/thomas/Principals/FL/2g.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8f9d83a-b411-45df-a6e1-ea5c8fcfd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_threshold_test(model, train_data):\n",
    "    device='cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse = list()\n",
    "    threshold_func = nn.MSELoss(reduction=\"none\")\n",
    "    for batch_idx, x in enumerate(train_data):\n",
    "        x = x.to(device).float()\n",
    "        diff = threshold_func(model(x), x)\n",
    "        mse.append(diff)\n",
    "    mse_global = torch.cat(mse).mean(dim=1)\n",
    "    return mse_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8012e2-003a-40b0-b57b-477d83997146",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for id in input_d.keys():\n",
    "    res[id] = {}\n",
    "    input_size = input_d[id]['train_data'].shape[1]\n",
    "    sample_num = input_d[id]['train_data'].shape[0]\n",
    "    print(f\"input size {input_size}, num samples {sample_num}\")\n",
    "    model = AutoEncoder(input_size)\n",
    "    learning_rate =  0.03#0.03\n",
    "    device='cpu'\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    epoch_loss = []\n",
    "    train_data = torch.utils.data.DataLoader(input_d[id]['train_data'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "        for batch_idx, x in enumerate(train_data):\n",
    "            x = x.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            decode = model(x)\n",
    "            loss = criterion(decode, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "        print(\"Client \\tEpoch: {}\\tLoss: {:.6f}\".format(epoch, sum(epoch_loss) / len(epoch_loss))) \n",
    "    threshold = get_local_threshold_test(model, train_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "714c8b63-436e-4a03-8135-c98c197e9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = torch.mean(threshold) + 3* torch.std(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820d8a0-bbe0-4bd7-a349-377ad08685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c84152bf-a97d-4413-8eab-d3eafa9c8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = threshold.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edfb76-c03d-4615-9d5b-d616f3293c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(r)*2 + np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23f587-ff39-4acb-a6a7-6a78ffa31718",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca39922-f449-4f43-a511-24bc354f2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r < (np.mean(r) + 1*np.std(r))].shape[0] / r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7efc193-bf9b-42d3-aec8-b1ac0689a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r < (np.mean(r) + 2*np.std(r))].shape[0] / r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe114f40-560f-45ed-85db-b220cb9d90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r < (np.mean(r) + 3*np.std(r))].shape[0] / r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da73db-750b-401d-ac03-464e12e572f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(threshold.detach().numpy())\n",
    "plt.show()\n",
    "plt.savefig(\"/data/thomas/Principals/FL/mse.pdf\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
